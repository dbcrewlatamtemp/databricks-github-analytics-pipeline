# **Semana 2: Transformaciones de Datos y CreaciÃ³n de la Capa Silver**

[![Fase 1](https://img.shields.io/badge/Fase%201-Esquemas%20y%20Limpieza-blue?style=for-the-badge)](./fase-1-esquemas-limpieza.md)
[![Fase 2](https://img.shields.io/badge/Fase%202-Transformaciones%20PySpark-green?style=for-the-badge)](./fase-2-transformaciones-pyspark.md)
[![Fase 3](https://img.shields.io/badge/Fase%203-Capa%20Silver-gold?style=for-the-badge)](./fase-3-capa-silver.md)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![Delta Lake](https://img.shields.io/badge/Delta%20Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)](https://delta.io/)

## **ğŸ¯ Objetivo de la Semana**

Transformar los datos en bruto de GitHub (Capa Bronze) en datos limpios, estructurados y optimizados para anÃ¡lisis (Capa Silver). AprenderÃ¡s las tÃ©cnicas fundamentales de **Data Engineering** usando **PySpark**, **Delta Lake** y **esquemas evolutivos** para crear pipelines de transformaciÃ³n robustos y escalables.

---

## **ğŸ“š Contenido de la Semana**

### **[ğŸ”§ Fase 1: Esquemas y Limpieza de Datos](./fase-1-esquemas-limpieza.md)**
**â±ï¸ DuraciÃ³n estimada:** 2-3 horas

#### **ğŸ› ï¸ Lo que vas a hacer:**
- âœ… Analizar la estructura de datos JSON de GitHub Archive
- âœ… Definir esquemas PySpark para eventos de GitHub
- âœ… Implementar validaciones de calidad de datos
- âœ… Limpiar y normalizar campos inconsistentes
- âœ… Manejar valores nulos y datos corruptos

#### **ğŸ“ Habilidades que desarrollarÃ¡s:**
- AnÃ¡lisis exploratorio de datos (EDA) con PySpark
- DefiniciÃ³n de esquemas estructurados
- TÃ©cnicas de limpieza de datos
- Validaciones de Data Quality
- Manejo de datos semi-estructurados (JSON)

---

### **[âš¡ Fase 2: Transformaciones Avanzadas con PySpark](./fase-2-transformaciones-pyspark.md)**
**â±ï¸ DuraciÃ³n estimada:** 3-4 horas

#### **ğŸ› ï¸ Lo que vas a hacer:**
- âœ… Transformar eventos JSON en tablas relacionales
- âœ… Extraer informaciÃ³n de payloads complejos
- âœ… Crear dimensiones de tiempo (timestamp parsing)
- âœ… Normalizar datos de usuarios y repositorios
- âœ… Implementar transformaciones avanzadas con SQL

#### **ğŸ“ Habilidades que desarrollarÃ¡s:**
- PySpark DataFrames y SQL avanzado
- Transformaciones de datos complejas
- Parsing de timestamps y fechas
- NormalizaciÃ³n de datos
- OptimizaciÃ³n de queries distribuidas

---

### **[ğŸ¥ˆ Fase 3: ImplementaciÃ³n de la Capa Silver](./fase-3-capa-silver.md)**
**â±ï¸ DuraciÃ³n estimada:** 2-3 horas

#### **ğŸ› ï¸ Lo que vas a hacer:**
- âœ… Configurar Delta Lake para ACID transactions
- âœ… Crear tablas Silver particionadas por fecha
- âœ… Implementar merge/upsert operations
- âœ… Optimizar storage con Z-Order clustering
- âœ… Crear pipeline de transformaciÃ³n automatizado

#### **ğŸ“ Habilidades que desarrollarÃ¡s:**
- Delta Lake para lakehouse architecture
- Particionado eficiente de datos
- Operaciones ACID en data lakes
- OptimizaciÃ³n de storage y queries
- Pipelines de transformaciÃ³n escalables

---

## **ğŸ—ï¸ Arquitectura que Vas a Construir**

```mermaid
graph TB
    subgraph "Semana 2 - Pipeline Silver"
        A[ğŸ¥‰ Bronze Layer<br/>Raw JSON Files] --> B[ğŸ” Schema Analysis<br/>Data Profiling]
        B --> C[ğŸ§¹ Data Cleaning<br/>Quality Validation]
        C --> D[âš¡ PySpark Transformations<br/>Complex Queries]
        D --> E[ğŸ¥ˆ Silver Layer<br/>Structured Tables]
        
        F[ğŸ“Š Delta Lake<br/>ACID Transactions] --> E
        G[ğŸ—‚ï¸ Partitioning<br/>Date-based] --> E
        H[âš¡ Z-Order Optimization<br/>Query Performance] --> E
    end
    
    subgraph "Data Quality Gates"
        I[ğŸ“ˆ Row Count Validation]
        J[ğŸ” Schema Validation]
        K[ğŸ“Š Business Rules Check]
        L[âš ï¸ Anomaly Detection]
    end
    
    C --> I
    C --> J
    C --> K
    C --> L
    
    style E fill:#C0C0C0,stroke:#808080,stroke-width:3px
    style A fill:#CD7F32,stroke:#8B4513,stroke-width:2px
```

### **ğŸ¯ Enfoque de la Semana 2:**
- **ğŸ¥ˆ Silver Layer:** Datos limpios y estructurados
- **âš¡ PySpark:** Transformaciones distribuidas
- **ğŸ—ƒï¸ Delta Lake:** Storage optimizado con ACID
- **ğŸ“Š Data Quality:** Validaciones y mÃ©tricas

---

## **ğŸ“Š Transformaciones de Datos**

### **ğŸ” AnÃ¡lisis de Eventos GitHub (Input):**
```json
{
  "id": "12345678901",
  "type": "PushEvent",
  "actor": {
    "id": 123456,
    "login": "octocat",
    "display_login": "octocat"
  },
  "repo": {
    "id": 654321,
    "name": "octocat/Hello-World"
  },
  "created_at": "2025-01-01T00:00:00Z",
  "payload": {
    "commits": [
      {
        "sha": "abc123def456...",
        "message": "Update README.md"
      }
    ]
  }
}
```

### **ğŸ¥ˆ Tabla Silver Resultante (Output):**
| event_id | event_type | user_id | username | repo_id | repo_name | event_date | hour | commits_count | main_language |
|----------|------------|---------|----------|---------|-----------|------------|------|---------------|---------------|
| 12345678901 | PushEvent | 123456 | octocat | 654321 | octocat/Hello-World | 2025-01-01 | 0 | 1 | Python |

### **ğŸ“ˆ MÃ©tricas de TransformaciÃ³n:**
- **ReducciÃ³n de volumen:** ~40-60% (JSON â†’ Columnar)
- **Mejora de queries:** 10-50x mÃ¡s rÃ¡pido
- **Data quality:** 99%+ eventos vÃ¡lidos
- **Particionado:** Por fecha para queries eficientes

---

## **ğŸš€ TecnologÃ­as de la Semana**

### **ğŸ› ï¸ Stack TecnolÃ³gico Principal:**

| TecnologÃ­a | PropÃ³sito | Nivel de Uso |
|------------|-----------|--------------|
| **PySpark** | Engine de transformaciones distribuidas | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **Delta Lake** | Storage format con ACID transactions | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª |
| **Spark SQL** | Queries y transformaciones declarativas | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª |
| **Python** | Desarrollo de transformaciones custom | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| **JSON Parsing** | ExtracciÃ³n de datos semi-estructurados | ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª |
| **Schema Evolution** | Manejo de cambios en estructura | ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª |

### **ğŸ“Š Conceptos Clave:**
- **DataFrames vs RDDs:** CuÃ¡ndo usar cada uno
- **Catalyst Optimizer:** OptimizaciÃ³n automÃ¡tica de queries
- **Partitioning Strategies:** DistribuciÃ³n eficiente de datos
- **Data Skew:** DetecciÃ³n y mitigaciÃ³n de desbalances
- **Broadcast Joins:** OptimizaciÃ³n de joins pequeÃ±os

---

## **ğŸš€ GuÃ­a de Inicio RÃ¡pido**

### **ğŸ“‹ Prerrequisitos de la Semana 2**
- [ ] **Semana 1 completada** exitosamente
- [ ] **Datos en capa Bronze** disponibles en Azure Storage
- [ ] **Databricks Serverless Warehouse** activo
- [ ] **Azure Storage** configurado y accesible
- [ ] **Conocimientos bÃ¡sicos** de SQL (recomendado)

### **ğŸ”„ Flujo de Trabajo Semana 2**

1. **ğŸ” [Comenzar con Fase 1](./fase-1-esquemas-limpieza.md)**
   - Analizar datos Bronze existentes
   - Definir esquemas y validaciones

2. **âš¡ [Continuar con Fase 2](./fase-2-transformaciones-pyspark.md)**
   - Implementar transformaciones complejas
   - Optimizar performance de queries

3. **ğŸ¥ˆ [Finalizar con Fase 3](./fase-3-capa-silver.md)**
   - Crear tablas Silver con Delta Lake
   - Automatizar pipeline de transformaciÃ³n

4. **âœ… Verificar resultados**
   - Validar calidad de datos Silver
   - Confirmar optimizaciones de performance

---

## **ğŸ“ Checklist de Progreso Semanal**

### **ğŸ” Fase 1: Esquemas y Limpieza**
- [ ] **AnÃ¡lisis Exploratorio de Datos**
  - [ ] Datos Bronze cargados en DataFrames
  - [ ] Estructura JSON analizada y documentada
  - [ ] Esquemas PySpark definidos por tipo de evento
  - [ ] EstadÃ­sticas bÃ¡sicas calculadas

- [ ] **Limpieza y ValidaciÃ³n**
  - [ ] Registros corruptos identificados y removidos
  - [ ] Campos nulos manejados apropiadamente
  - [ ] Validaciones de data quality implementadas
  - [ ] Transformaciones de limpieza aplicadas

### **âš¡ Fase 2: Transformaciones PySpark**
- [ ] **Transformaciones Core**
  - [ ] Eventos JSON parseados a columnas estructuradas
  - [ ] Timestamps convertidos a tipos de fecha
  - [ ] Payloads complejos extraÃ­dos (commits, issues, etc.)
  - [ ] Campos calculados agregados (hour, day_of_week, etc.)

- [ ] **NormalizaciÃ³n de Datos**
  - [ ] InformaciÃ³n de usuarios normalizada
  - [ ] Datos de repositorios estructurados
  - [ ] MÃ©tricas de eventos calculadas
  - [ ] Transformaciones SQL avanzadas aplicadas

### **ğŸ¥ˆ Fase 3: Capa Silver**
- [ ] **ConfiguraciÃ³n Delta Lake**
  - [ ] Delta Lake habilitado en Azure Storage
  - [ ] Tablas Silver creadas con esquemas apropiados
  - [ ] Particionado por fecha implementado
  - [ ] Z-Order clustering configurado

- [ ] **Pipeline de TransformaciÃ³n**
  - [ ] Proceso ETL automatizado Bronze â†’ Silver
  - [ ] Operaciones merge/upsert funcionando
  - [ ] Validaciones de calidad en pipeline
  - [ ] MÃ©tricas de performance documentadas

- [ ] **ValidaciÃ³n Final**
  - [ ] Datos Silver verificados en Azure Portal
  - [ ] Queries de ejemplo ejecutadas exitosamente
  - [ ] Performance comparado con Bronze
  - [ ] Pipeline listo para automatizaciÃ³n

---

## **ğŸ“ Conocimientos y Habilidades Adquiridas**

### **ğŸ’¡ Conceptos de Data Engineering Avanzados**
- **ğŸ—ï¸ Lakehouse Architecture:** CombinaciÃ³n de data lakes y data warehouses
- **ğŸ”„ ETL vs ELT:** Diferencias y cuÃ¡ndo usar cada enfoque
- **ğŸ“Š Data Quality Engineering:** MÃ©tricas, validaciones y monitoreo
- **âš¡ Distributed Computing:** ParalelizaciÃ³n y optimizaciÃ³n de workloads

### **ğŸ› ï¸ Habilidades TÃ©cnicas EspecÃ­ficas**
- **ğŸ PySpark Avanzado:** DataFrames, SQL, y optimizaciones
- **ğŸ—ƒï¸ Delta Lake:** ACID transactions, time travel, schema evolution
- **ğŸ“ˆ Performance Tuning:** Partitioning, caching, broadcasting
- **ğŸ” Data Profiling:** AnÃ¡lisis estadÃ­stico y detecciÃ³n de anomalÃ­as

### **ğŸ¢ Aplicaciones Empresariales**
- **ğŸ“Š Data Pipeline Design:** Arquitectura de pipelines escalables
- **ğŸ”§ Data Transformation:** TÃ©cnicas de limpieza y normalizaciÃ³n
- **ğŸ“ˆ Performance Optimization:** Queries eficientes en big data
- **ğŸ›¡ï¸ Data Governance:** Calidad, linaje y documentaciÃ³n

---

## **ğŸ” Casos de Uso de la Semana**

### **ğŸ“Š AnÃ¡lisis que HabilitarÃ¡s:**
- **Developer Activity Patterns:** AnÃ¡lisis temporal de commits
- **Repository Trends:** Tendencias de popularidad y actividad
- **Language Analytics:** DistribuciÃ³n y evoluciÃ³n de lenguajes
- **Collaboration Networks:** Patrones de contribuciÃ³n entre usuarios

### **ğŸ¯ MÃ©tricas que CrearÃ¡s:**
- **Commits por hora/dÃ­a/mes** para anÃ¡lisis temporal
- **Actividad por repositorio** para ranking de popularidad
- **Contribuciones por desarrollador** para anÃ¡lisis de productividad
- **Eventos por tipo** para entender patrones de uso de GitHub

---

## **ğŸ” Troubleshooting y Soporte**

### **â“ Problemas Comunes y Soluciones**

| **Problema** | **SÃ­ntoma** | **SoluciÃ³n** |
|--------------|-------------|--------------|
| **JSON parsing errors** | "Malformed JSON" | Implementar try-catch y logging robusto |
| **Schema evolution issues** | "Column not found" | Usar schema merging y campos opcionales |
| **Performance lento** | Queries toman mucho tiempo | Revisar partitioning y optimizar joins |
| **Memory errors** | "OutOfMemory exception" | Ajustar configuraciÃ³n de Spark y partitioning |
| **Delta Lake errors** | "Concurrent modification" | Implementar retry logic y optimistic concurrency |

### **ğŸ†˜ DÃ³nde Buscar Ayuda**
- **ğŸ“‹ Spark UI:** AnÃ¡lisis de jobs y stages
- **ğŸ” Delta Lake Logs:** Monitoring de transacciones
- **ğŸ“š DocumentaciÃ³n:** Enlaces especÃ­ficos en cada fase
- **ğŸ’¬ GitHub Issues:** Reportar problemas del proyecto

---

## **ğŸ“ˆ PreparaciÃ³n para Semana 3**

### **ğŸ¯ Vista Previa: Agregaciones y Capa Gold**
- **Objetivo:** Crear mÃ©tricas de negocio y tablas analÃ­ticas
- **TecnologÃ­as:** Spark SQL avanzado, Window functions, Aggregations
- **DuraciÃ³n:** 3-4 horas
- **Prerrequisito:** Semana 2 completada con datos Silver funcionando

### **ğŸ”¥ Lo que viene:**
- Agregaciones temporales (daily, weekly, monthly)
- MÃ©tricas de negocio avanzadas
- Tablas dimensionales para BI
- OptimizaciÃ³n para consultas analÃ­ticas
- PreparaciÃ³n de datos para dashboards

---

## **ğŸ“š Recursos de Aprendizaje**

### **ğŸ“– DocumentaciÃ³n Oficial**
- [**PySpark Documentation**](https://spark.apache.org/docs/latest/api/python/)
- [**Delta Lake Guide**](https://docs.delta.io/latest/index.html)
- [**Databricks PySpark Reference**](https://docs.databricks.com/pyspark/index.html)
- [**Spark SQL Guide**](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### **ğŸ¥ Videos Recomendados**
- PySpark DataFrames Deep Dive
- Delta Lake Architecture Overview
- Spark Performance Tuning
- Data Quality Best Practices

### **ğŸ“ ArtÃ­culos Complementarios**
- Medallion Architecture Implementation
- JSON Processing at Scale
- Data Partitioning Strategies
- Modern ETL Design Patterns

---

## **ğŸ† CertificaciÃ³n de Completitud Semana 2**

Al finalizar esta semana exitosamente, habrÃ¡s:

âœ… **Dominado PySpark** para transformaciones distribuidas  
âœ… **Implementado Delta Lake** para storage optimizado  
âœ… **Creado pipelines ETL** robustos y escalables  
âœ… **Aplicado data quality** engineering  
âœ… **Optimizado performance** de queries en big data  

### **ğŸ–ï¸ Habilidades Validadas:**
- **Advanced Data Engineering** con PySpark y Delta Lake
- **ETL Pipeline Development** para producciÃ³n
- **Data Quality Engineering** y validaciones
- **Performance Optimization** en sistemas distribuidos
- **Lakehouse Architecture** implementation

---

## **ğŸš€ Â¿Listo para Empezar?**

### **ğŸ“ Tu PrÃ³ximo Paso:**

<div align="center">

**[ğŸ” Comenzar con Fase 1: Esquemas y Limpieza](./fase-1-esquemas-limpieza.md)**

*Tiempo estimado: 2-3 horas*

---

**ğŸ’¡ Consejo:** Esta semana es mÃ¡s tÃ©cnica que la anterior. TÃ³mate tu tiempo para entender cada concepto antes de avanzar. La calidad de tu capa Silver determinarÃ¡ el Ã©xito de todas las semanas siguientes.

</div>

---

**[â¬…ï¸ Regresar a Semana 1](../semana-1/README.md)** | **[â¡ï¸ Ir a Fase 1](./fase-1-esquemas-limpieza.md)** | **[ğŸ  Proyecto Principal](../README.md)**