{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321b3496",
   "metadata": {},
   "source": [
    "\n",
    "# DBT + Airflow para **Silver (Dev)** en Databricks  \n",
    "**Objetivo:** levantar un entorno paralelo de *silver desarrollo* (vía **dbt**) sin tocar *silver producción*, y orquestarlo con **Airflow**.\n",
    "\n",
    "> Este notebook es una guía ejecutable para que el equipo configure dbt con Databricks (adapter oficial) y corra *pipelines* de *silver dev* en paralelo a *silver prod*. Incluye ejemplos de `profiles.yml`, `dbt_project.yml`, un modelo incremental y un DAG de Airflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ac0eb",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Requisitos previos (resumen ejecutivo)\n",
    "- **Databricks** con Unity Catalog habilitado (ideal) o al menos un Cluster/SQL Warehouse accesible.\n",
    "- **Adapter dbt-databricks** (recomendado) o **dbt-spark** (si no usas Databricks SQL).  \n",
    "- **GitHub** (código) + **CI/CD runner** o máquina donde correr `dbt` (puede ser un Job en Databricks o un servidor con Airflow).\n",
    "- **Esquemas aislados**: `silver` (prod) y `silver_dev` (desarrollo).  \n",
    "  - Alternativa: usar `schema: silver__{{ env_var('DBT_ENV_NAME', 'dev') }}` para esquemas por entorno.\n",
    "- **Credenciales**: Token personal o Service Principal de Databricks, host del workspace y (si aplica) catalog/database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77171c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Instalación del adapter de dbt\n",
    "En el entorno donde correrá dbt (local, runner, contenedor o VM):\n",
    "\n",
    "```bash\n",
    "# Opción 1: Databricks (recomendada)\n",
    "pip install dbt-databricks\n",
    "\n",
    "# Opción 2: Spark genérico\n",
    "pip install dbt-spark[PyHive]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036b03f",
   "metadata": {},
   "source": [
    "\n",
    "## 3) `profiles.yml` (dos *targets*: `prod` y `dev`)\n",
    "Crea `~/.dbt/profiles.yml` (o usa variable `DBT_PROFILES_DIR`) similar a esto.  \n",
    "**Nota:** abajo también te dejo un archivo descargable `profiles.yml.example`.\n",
    "\n",
    "```yaml\n",
    "databricks_project:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    prod:\n",
    "      type: databricks\n",
    "      catalog: main                    # Ajusta a tu catalog (Unity Catalog)\n",
    "      schema: silver                   # Esquema producción\n",
    "      host: https://<tu-workspace>.cloud.databricks.com\n",
    "      http_path: /sql/1.0/warehouses/<WAREHOUSE_ID>   # Si usas SQL Warehouse\n",
    "      token: \"{{ env_var('DATABRICKS_TOKEN') }}\"\n",
    "      threads: 8\n",
    "\n",
    "    dev:\n",
    "      type: databricks\n",
    "      catalog: main\n",
    "      schema: silver_dev               # Esquema desarrollo\n",
    "      host: https://<tu-workspace>.cloud.databricks.com\n",
    "      http_path: /sql/1.0/warehouses/<WAREHOUSE_ID>\n",
    "      token: \"{{ env_var('DATABRICKS_TOKEN') }}\"\n",
    "      threads: 8\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6cc59",
   "metadata": {},
   "source": [
    "\n",
    "## 4) `dbt_project.yml`\n",
    "Proyecto mínimo para aislar *silver dev* y *silver prod* por target.  \n",
    "Archivo de ejemplo también disponible como `dbt_project.yml.example`:\n",
    "\n",
    "```yaml\n",
    "name: databricks_silver\n",
    "version: 1.0.0\n",
    "config-version: 2\n",
    "\n",
    "profile: databricks_project\n",
    "\n",
    "model-paths: [\"models\"]\n",
    "seed-paths: [\"seeds\"]\n",
    "test-paths: [\"tests\"]\n",
    "macro-paths: [\"macros\"]\n",
    "\n",
    "models:\n",
    "  databricks_silver:\n",
    "    +materialized: incremental\n",
    "    +on_schema_change: append_new_columns\n",
    "    +file_format: delta\n",
    "    +tags: [\"silver\"]\n",
    "\n",
    "    # Puedes sobreescribir por subcarpetas (silver/, gold/, etc.)\n",
    "    silver:\n",
    "      +materialized: incremental\n",
    "      +schema: \"{{ target.schema }}\"   # respeta el schema de cada target (prod/dev)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e790b7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Estructura de carpetas sugerida\n",
    "```\n",
    "dbt_project_root/\n",
    "├─ models/\n",
    "│  └─ silver/\n",
    "│     ├─ stg_eventos.sql\n",
    "│     └─ usuarios_incremental.sql\n",
    "├─ seeds/\n",
    "├─ macros/\n",
    "├─ dbt_project.yml\n",
    "└─ profiles.yml  (o en ~/.dbt/profiles.yml)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2e386",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Modelo incremental de ejemplo (Delta + deduplicación)\n",
    "Guarda como `models/silver/usuarios_incremental.sql` (también dejo el archivo listo para descargar).\n",
    "\n",
    "```sql\n",
    "{{ config(\n",
    "    materialized='incremental',\n",
    "    unique_key='user_id',\n",
    "    incremental_strategy='merge'\n",
    ") }}\n",
    "\n",
    "with fuente as (\n",
    "    -- Ejemplo: tabla BRONZE ya creada por tus notebooks de Databricks\n",
    "    select\n",
    "        cast(user_id as string)         as user_id,\n",
    "        lower(trim(user_name))          as user_name,\n",
    "        cast(event_ts as timestamp)     as event_ts\n",
    "    from {{ source('bronze', 'events_raw') }}\n",
    "), dedup as (\n",
    "    select *\n",
    "    from (\n",
    "      select\n",
    "        *,\n",
    "        row_number() over (partition by user_id order by event_ts desc) as rn\n",
    "      from fuente\n",
    "    ) x\n",
    "    where rn = 1\n",
    ")\n",
    "\n",
    "select user_id, user_name, event_ts\n",
    "from dedup\n",
    "\n",
    "{% if is_incremental() %}\n",
    "  -- En modo incremental, trae solo nuevos/actualizados\n",
    "  where event_ts > coalesce((select max(event_ts) from {{ this }}), '1900-01-01')\n",
    "{% endif %}\n",
    "```\n",
    "> **Nota:** Define tus *sources* en `models/sources.yml` apuntando a tus tablas/volúmenes reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883dc35",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Comandos clave de dbt\n",
    "```bash\n",
    "# Inicializa proyecto (si es nuevo)\n",
    "dbt init databricks_silver\n",
    "\n",
    "# Prueba conexión\n",
    "dbt debug --target dev\n",
    "\n",
    "# Documentación local (opcional)\n",
    "dbt docs generate && dbt docs serve\n",
    "\n",
    "# Correr en DEV (silver_dev)\n",
    "dbt run --select tag:silver --target dev\n",
    "dbt test --select tag:silver --target dev\n",
    "\n",
    "# Correr en PROD (silver)\n",
    "dbt run --select tag:silver --target prod\n",
    "dbt test --select tag:silver --target prod\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1c250",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Orquestación con Airflow (BashOperator)\n",
    "El DAG que te dejo (`dags/airflow_dag_dbt_silver.py`) corre `dbt run/test` con *targets* `dev` y `prod`.  \n",
    "Requisitos en el *worker*: Python con `dbt-databricks` instalado y variables de entorno (`DATABRICKS_TOKEN`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515df19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Opcional) Verifica variables de entorno requeridas para dbt-databricks\n",
    "import os\n",
    "vars_needed = [\"DATABRICKS_TOKEN\"]\n",
    "missing = [v for v in vars_needed if not os.environ.get(v)]\n",
    "print(\"Variables faltantes:\", missing if missing else \"OK (todas presentes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fdd34",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Buenas prácticas para “Silver paralelo”\n",
    "- Mantén **aislamiento por esquema** (`silver` vs `silver_dev`) o por `catalog` si aplica UC.  \n",
    "- Usa **tags** (`tag:silver`) para seleccionar modelos en jobs y CI/CD.  \n",
    "- Aplica **pruebas dbt** (unique, not_null, relationships) antes de promover a *prod*.  \n",
    "- En Databricks, prioriza **formato Delta** y `ZORDER`/`OPTIMIZE` en tablas pesadas.  \n",
    "- Versiona tus `sources` y modelos con *pull requests* y *code reviews*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
