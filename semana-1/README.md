
# **Semana 1: ConfiguraciÃ³n del Entorno y CreaciÃ³n de la Capa Bronze**

[![Fase 1](https://img.shields.io/badge/Fase%201-ConfiguraciÃ³n-blue?style=for-the-badge)](./fase-1-configuracion.md)
[![Fase 2](https://img.shields.io/badge/Fase%202-Capa%20Bronze-green?style=for-the-badge)](./fase-2-capa-bronze.md)
[![Azure](https://img.shields.io/badge/Azure-Storage-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com/)
[![Databricks](https://img.shields.io/badge/Databricks-Community-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://community.cloud.databricks.com/)

## **ğŸ¯ Objetivo de la Semana**

Al finalizar esta semana, habrÃ¡s configurado tu entorno completo de Data Engineering y habrÃ¡s construido tu primer pipeline de datos real. TendrÃ¡s un sistema funcional que descarga, procesa y almacena eventos de GitHub usando **Databricks** y **Azure Storage** con la arquitectura **Medallion (Bronze-Silver-Gold)**.

-----

## **ğŸ“š Contenido de la Semana**

### **[ğŸ”§ Fase 1: ConfiguraciÃ³n del Entorno](./fase-1-configuracion.md)**

**â±ï¸ DuraciÃ³n estimada:** 2-3 horas

#### **ğŸ› ï¸ Lo que vas a hacer:**

- âœ… Registrarte en Databricks Community Edition (gratuito)
- âœ… Configurar un clÃºster computacional
- âœ… Integrar Azure Storage Account como Data Lake
- âœ… Configurar Databricks Secrets para seguridad
- âœ… Crear la estructura Medallion Architecture

#### **ğŸ“ Habilidades que desarrollarÃ¡s:**

- ConfiguraciÃ³n de entornos cloud
- GestiÃ³n de credenciales seguras
- IntegraciÃ³n de servicios Azure
- Conceptos de Data Lake y almacenamiento

-----

### **[ğŸ“Š Fase 2: Carga y Almacenamiento (Capa Bronze)](./fase-2-capa-bronze.md)**

**â±ï¸ DuraciÃ³n estimada:** 1-2 horas

#### **ğŸ› ï¸ Lo que vas a hacer:**

- âœ… Crear tu primer notebook de Data Engineering
- âœ… Descargar datos reales de GitHub Archive
- âœ… Procesar archivos comprimidos (.gz)
- âœ… Cargar datos a Azure Storage (Capa Bronze)
- âœ… Validar y verificar la calidad de datos

#### **ğŸ“ Habilidades que desarrollarÃ¡s:**

- Ingesta automatizada de datos
- Manejo de APIs externas
- Procesamiento de archivos
- ValidaciÃ³n de pipelines de datos

-----

## **ğŸ—ï¸ Arquitectura que Vas a Construir**

```mermaid
graph LR
    subgraph "Semana 1 - Pipeline Bronze"
        A[ğŸ™ GitHub Archive] --> B[ğŸ““ Databricks Notebook]
        B --> C[â˜ï¸ Azure Blob Storage]
        C --> D[ğŸ¥‰ Bronze Layer]
        
        E[ğŸ” Azure Storage Account] --> C
        F[ğŸ—ï¸ Databricks Secrets] --> B
    end
    
    subgraph "PrÃ³ximas Semanas"
        D --> G[ğŸ¥ˆ Silver Layer]
        G --> H[ğŸ¥‡ Gold Layer]
        H --> I[ğŸ“Š Analytics]
    end
    
    style D fill:#CD7F32,stroke:#8B4513,stroke-width:3px
    style G fill:#C0C0C0,stroke:#808080,stroke-width:2px
    style H fill:#FFD700,stroke:#FFA500,stroke-width:2px
```

### **ğŸ¯ Enfoque de la Semana 1:**

- **ğŸ¥‰ Bronze Layer:** Datos en formato original/crudo
- **ğŸ“¦ Almacenamiento:** Azure Blob Storage como Data Lake
- **ğŸ”„ Procesamiento:** Descarga y carga automatizada
- **ğŸ›¡ï¸ Seguridad:** Credenciales protegidas con Secrets

-----

## **ğŸ“Š Datos del Proyecto**

### **ğŸ” Fuente de Datos: GitHub Archive**

- **URL:** [gharchive.org](https://www.gharchive.org/)
- **Contenido:** Eventos pÃºblicos de GitHub cada hora
- **Formato:** JSON comprimido (.gz)
- **Volumen:** ~10-50 MB por archivo

### **ğŸ“… Datos de Esta Semana:**

- **PerÃ­odo:** 2 dÃ­as (01-01-2025 y 02-01-2025)
- **Horas por dÃ­a:** 3 horas (0:00, 1:00, 2:00)
- **Total archivos:** 6 archivos JSON
- **Volumen total:** ~60-300 MB

### **ğŸ¯ Tipos de Eventos GitHub:**

|Evento              |DescripciÃ³n               |Frecuencia|
|--------------------|--------------------------|----------|
|**PushEvent**       |Commits subidos a repos   |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥     |
|**CreateEvent**     |CreaciÃ³n de repos/branches|ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª     |
|**PullRequestEvent**|Pull requests             |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª     |
|**IssuesEvent**     |Issues y comentarios      |ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª     |
|**WatchEvent**      |Stars en repositorios     |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª     |
|**ForkEvent**       |Forks de repositorios     |ğŸ”¥ğŸ”¥âšªâšªâšª     |

-----

## **ğŸš€ GuÃ­a de Inicio RÃ¡pido**

### **ğŸ“‹ Prerrequisitos**

- [ ] **Cuenta Azure** con Storage Account
- [ ] **Permisos Azure:** Storage Account Contributor + Storage Blob Data Contributor
- [ ] **Navegador web** actualizado
- [ ] **ConexiÃ³n a internet** estable
- [ ] **2-4 horas** de tiempo disponible

### **ğŸ”„ Flujo de Trabajo**

1. **ğŸ”§ [Comenzar con Fase 1](./fase-1-configuracion.md)**
- Configurar Databricks y Azure Storage
- Crear secrets y estructura del proyecto
1. **ğŸ“Š [Continuar con Fase 2](./fase-2-capa-bronze.md)**
- Implementar ingesta de datos
- Cargar datos a la capa Bronze
1. **âœ… Verificar resultados**
- Validar datos en Azure Portal
- Confirmar pipeline funcionando
1. **ğŸ¯ Prepararse para Semana 2**
- Datos listos para transformaciÃ³n
- Entorno configurado completamente

-----

## **ğŸ“ Checklist de Progreso Semanal**

### **ğŸ”§ Fase 1: ConfiguraciÃ³n del Entorno**

- [ ] **Databricks Community Edition**
  - [ ] Cuenta creada y verificada por email
  - [ ] Workspace accesible y explorado
  - [ ] Interfaz principal navegada
- [ ] **ClÃºster Computacional**
  - [ ] ClÃºster `cluster-proyecto-github` creado
  - [ ] Runtime LTS mÃ¡s reciente seleccionado
  - [ ] Estado activo confirmado (cÃ­rculo verde)
- [ ] **Azure Storage Account**
  - [ ] Grupo de recursos `rg-proyecto-github` accedido
  - [ ] Permisos verificados en Azure Portal
  - [ ] Storage Account identificado y accedido
  - [ ] Credenciales obtenidas (nombre + clave)
- [ ] **Databricks Secrets**
  - [ ] Scope `azure-storage-secrets` creado
  - [ ] Secret `storage-account-name` agregado
  - [ ] Secret `storage-account-key` agregado
  - [ ] ConexiÃ³n probada exitosamente
- [ ] **Estructura Medallion**
  - [ ] Contenedor `alopez-proyecto-gh` creado
  - [ ] Carpeta `bronze/` implementada
  - [ ] Carpeta `silver/` preparada
  - [ ] Carpeta `gold/` preparada

### **ğŸ“Š Fase 2: Capa Bronze**

- [ ] **Notebook de Ingesta**
  - [ ] Carpeta `Proyecto-GitHub-Analytics` creada
  - [ ] Notebook `01_Ingesta_Bronze_Azure` creado
  - [ ] ClÃºster adjuntado correctamente
- [ ] **ConfiguraciÃ³n del Pipeline**
  - [ ] LibrerÃ­as de Azure instaladas
  - [ ] Cliente de storage configurado
  - [ ] Funciones utilitarias implementadas
  - [ ] URLs de GitHub Archive configuradas
- [ ] **Proceso de Ingesta**
  - [ ] Descarga automÃ¡tica funcionando
  - [ ] DescompresiÃ³n de archivos .gz
  - [ ] Carga a Azure Storage exitosa
  - [ ] 6 archivos procesados correctamente
- [ ] **ValidaciÃ³n y VerificaciÃ³n**
  - [ ] Archivos visibles en Azure Portal
  - [ ] Contenido JSON validado
  - [ ] EstadÃ­sticas de archivos obtenidas
  - [ ] Rutas preparadas para siguiente notebook

-----

## **ğŸ“ Conocimientos y Habilidades Adquiridas**

### **ğŸ’¡ Conceptos de Data Engineering**

- **ğŸ“Š Medallion Architecture:** ComprensiÃ³n de capas Bronze-Silver-Gold
- **ğŸ—ï¸ Data Lake:** DiseÃ±o de almacenamiento escalable
- **ğŸ”„ ETL Pipelines:** Procesos Extract-Transform-Load
- **ğŸ›¡ï¸ Data Security:** GestiÃ³n segura de credenciales

### **ğŸ› ï¸ Habilidades TÃ©cnicas**

- **â˜ï¸ Cloud Computing:** Azure Storage y Databricks
- **ğŸ Python:** ProgramaciÃ³n para Data Engineering
- **ğŸ“¦ APIs:** IntegraciÃ³n con servicios externos
- **ğŸ”§ DevOps:** ConfiguraciÃ³n de entornos de desarrollo

### **ğŸ¢ Aplicaciones Empresariales**

- **ğŸ“ˆ Data Ingestion:** Procesos de carga masiva de datos
- **ğŸ” Data Validation:** VerificaciÃ³n de calidad de datos
- **âš™ï¸ Automation:** AutomatizaciÃ³n de tareas repetitivas
- **ğŸ“Š Analytics Foundation:** Base para anÃ¡lisis avanzados

-----

## **ğŸ“ Estructura de Archivos de la Semana**

```
semana-1/
â”œâ”€â”€ README.md                          # ğŸ“– Esta guÃ­a
â”œâ”€â”€ fase-1-configuracion.md           # ğŸ”§ Setup del entorno
â”œâ”€â”€ fase-2-capa-bronze.md             # ğŸ“Š ImplementaciÃ³n Bronze
â”‚
â””â”€â”€ notebooks/                        # ğŸ““ Notebooks para Databricks
    â”œâ”€â”€ 01_Setup_Azure_Storage.py     # ConfiguraciÃ³n inicial
    â”œâ”€â”€ 02_Ingesta_Bronze.py          # Pipeline de ingesta
    â””â”€â”€ 03_Validation_Bronze.py       # ValidaciÃ³n de datos
```

-----

## **ğŸ” Troubleshooting y Soporte**

### **â“ Problemas Comunes y Soluciones**

|**Problema**                 |**SÃ­ntoma**           |**SoluciÃ³n**                             |
|-----------------------------|----------------------|-----------------------------------------|
|**Error autenticaciÃ³n Azure**|â€œAccess deniedâ€       |Verificar permisos IAM en Azure Portal   |
|**Secrets no encontrados**   |â€œSecret not foundâ€    |Recrear scope y secrets en Databricks    |
|**ClÃºster no inicia**        |â€œStartingâ€¦â€ prolongado|Reintentar creaciÃ³n con runtime diferente|
|**Error descarga datos**     |â€œNetwork timeoutâ€     |Verificar conectividad y URLs de GitHub  |
|**Archivos no aparecen**     |Azure Storage vacÃ­o   |Revisar proceso de carga y credenciales  |

### **ğŸ†˜ DÃ³nde Buscar Ayuda**

- **ğŸ“‹ Logs Databricks:** Cluster â†’ Logs â†’ Driver Logs
- **â˜ï¸ Azure Portal:** Storage Account â†’ Monitoring â†’ Metrics
- **ğŸ“š DocumentaciÃ³n:** Enlaces en cada fase
- **ğŸ’¬ GitHub Issues:** Reportar problemas del proyecto

-----

## **ğŸ“ˆ PreparaciÃ³n para Semana 2**

### **ğŸ¯ Vista Previa: Transformaciones y Capa Silver**

- **Objetivo:** Limpiar y estructurar datos de GitHub
- **TecnologÃ­as:** PySpark, Delta Lake, Esquemas
- **DuraciÃ³n:** 3-4 horas
- **Prerrequisito:** Semana 1 completada exitosamente

### **ğŸ”¥ Lo que viene:**

- AplicaciÃ³n de esquemas estructurados
- Transformaciones avanzadas con PySpark
- Limpieza y validaciÃ³n de datos JSON
- OptimizaciÃ³n de performance y particionado
- PreparaciÃ³n de datos para anÃ¡lisis

-----

## **ğŸ“š Recursos de Aprendizaje**

### **ğŸ“– DocumentaciÃ³n Oficial**

- [**Databricks Community Edition**](https://docs.databricks.com/getting-started/community-edition.html)
- [**Azure Blob Storage Python SDK**](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)
- [**GitHub Archive Documentation**](https://www.gharchive.org/)
- [**Medallion Architecture Guide**](https://databricks.com/glossary/medallion-architecture)

### **ğŸ¥ Videos Recomendados**

- Databricks Community Edition Setup
- Azure Storage Account Configuration
- Introduction to Data Lakes
- Python for Data Engineering

### **ğŸ“ ArtÃ­culos Complementarios**

- Best Practices for Data Ingestion
- Cloud Security for Data Engineers
- JSON Data Processing Techniques
- Modern Data Architecture Patterns

-----

## **ğŸ† CertificaciÃ³n de Completitud**

Al finalizar esta semana exitosamente, habrÃ¡s:

âœ… **Configurado un entorno profesional** de Data Engineering  
âœ… **Implementado tu primer pipeline** de datos real  
âœ… **Trabajado con tecnologÃ­as enterprise** (Azure + Databricks)  
âœ… **Procesado datos del mundo real** (GitHub Archive)  
âœ… **Aplicado mejores prÃ¡cticas** de seguridad y arquitectura

### **ğŸ–ï¸ Habilidades Validadas:**

- **Cloud Data Engineering** con Azure y Databricks
- **Pipeline Development** con Python
- **Data Lake Architecture** con Medallion pattern
- **API Integration** y data ingestion
- **Security Best Practices** con secrets management

-----

## **ğŸš€ Â¿Listo para Empezar?**

### **ğŸ“ Tu PrÃ³ximo Paso:**

<div align="center">

**[ğŸ”§ Comenzar con Fase 1: ConfiguraciÃ³n del Entorno](./fase-1-configuracion.md)**

*Tiempo estimado: 2-3 horas*

-----

**ğŸ’¡ Consejo:** Dedica tiempo suficiente a cada fase y no tengas prisa. La configuraciÃ³n correcta del entorno es crucial para el Ã©xito de todo el proyecto.

</div>

-----

**[â¬…ï¸ Regresar al Proyecto Principal](../README.md)** | **[â¡ï¸ Ir a Fase 1](./fase-1-configuracion.md)**

