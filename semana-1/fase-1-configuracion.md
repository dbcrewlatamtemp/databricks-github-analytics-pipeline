# **Fase 1: ConfiguraciÃ³n del Entorno en Databricks**

## **IntroducciÃ³n**

Antes de poder construir cualquier pipeline de datos, necesitamos preparar nuestro espacio de trabajo. Esto implica registrarnos en Databricks Community Edition, crear un clÃºster computacional, y configurar Azure Storage Account como nuestro sistema de almacenamiento externo. Esta configuraciÃ³n nos permitirÃ¡ implementar la **Medallion Architecture** (Bronze-Silver-Gold) para nuestro proyecto de anÃ¡lisis de datos de GitHub.

-----

## **Paso 1.1: Registrarse en Databricks Community Edition**

La Community Edition es una versiÃ³n gratuita de Databricks, ideal para aprender y desarrollar proyectos personales.

### **Proceso de Registro:**

1. **Ve a la pÃ¡gina de registro:**
- Abre tu navegador y dirÃ­gete a: [Databricks Community Edition](https://community.cloud.databricks.com/login.html)
1. **Completa el formulario:**
- Rellena tus datos personales
- AsegÃºrate de usar un correo electrÃ³nico vÃ¡lido
- Crea una contraseÃ±a segura
1. **Selecciona la opciÃ³n â€œCommunity Editionâ€:**
- Cuando se te presente la opciÃ³n de elegir un plan
- Selecciona **â€œGet started with Community Editionâ€**
1. **Verifica tu correo electrÃ³nico:**
- RecibirÃ¡s un correo para confirmar tu cuenta
- Sigue las instrucciones para activar tu workspace

### **Material de Apoyo:**

- **DocumentaciÃ³n Oficial:** [GuÃ­a de inicio de Databricks](https://docs.databricks.com/getting-started/community-edition.html)

-----

## **Paso 1.2: Explorando la Interfaz de Databricks**

Una vez dentro de tu workspace, familiarÃ­zate con la interfaz principal.

### **Componentes Principales:**

|SecciÃ³n      |DescripciÃ³n                            |Uso en el Proyecto                |
|-------------|---------------------------------------|----------------------------------|
|**Workspace**|Organiza carpetas, notebooks y archivos|Crear estructura del proyecto     |
|**Compute**  |Gestiona clÃºsteres computacionales     |Crear y administrar el clÃºster    |
|**Data**     |Explora bases de datos y tablas        |Visualizar datos transformados    |
|**Jobs**     |Automatiza ejecuciÃ³n de notebooks      |Para etapas avanzadas del proyecto|

### **NavegaciÃ³n BÃ¡sica:**

- **MenÃº lateral izquierdo:** Acceso rÃ¡pido a todas las secciones
- **Ãrea principal:** Espacio de trabajo principal
- **Barra superior:** Configuraciones de usuario y workspace

-----

## **Paso 1.3: Creando tu Primer ClÃºster**

El clÃºster es el motor computacional que procesarÃ¡ tus datos. Sin Ã©l, los notebooks no pueden ejecutar cÃ³digo.

### **ConfiguraciÃ³n del ClÃºster:**

1. **Navega a la secciÃ³n â€œComputeâ€:**
- Haz clic en el Ã­cono **â€œComputeâ€** en el menÃº lateral
1. **Crear un nuevo clÃºster:**
- Haz clic en **â€œCreate Computeâ€**
1. **ConfiguraciÃ³n recomendada:**
- **Cluster Name:** `cluster-proyecto-github`
- **Databricks Runtime Version:** Selecciona la versiÃ³n **LTS** mÃ¡s reciente (no Beta)
- **Worker Type:** Mantener configuraciÃ³n por defecto
- **Driver Type:** Mantener configuraciÃ³n por defecto
- **Autoscaling:** Mantener habilitado
1. **Iniciar el clÃºster:**
- Haz clic en **â€œCreate Clusterâ€**
- El proceso toma entre 3-5 minutos
- **Estado listo:** CÃ­rculo verde junto al nombre

### **Limitaciones Community Edition:**

- âœ… **Permitido:** 1 clÃºster activo
- âœ… **Permitido:** Hasta 2 nodos worker
- âŒ **Limitado:** Sin acceso a funcionalidades premium
- âŒ **Limitado:** Sin almacenamiento persistente integrado

### **Material de Apoyo:**

- **DocumentaciÃ³n Oficial:** [GuÃ­a sobre Clusters en Databricks](https://docs.databricks.com/clusters/index.html)

-----

## **Paso 1.4: Configurando Azure Storage Account**

Dado que Databricks Community Edition no permite almacenamiento persistente integrado, configuraremos Azure Storage Account como nuestro sistema de almacenamiento externo.

### **0. ValidaciÃ³n del Acceso a Azure**

#### **VerificaciÃ³n de Permisos (Paso a Paso):**

1. **Acceder al grupo de recursos:**
   
   ```
   URL: https://portal.azure.com/#@tu-tenant.onmicrosoft.com/resource/subscriptions/tu-subscription-id/resourceGroups/rg-proyecto-github/overview
   ```
1. **Navegar a Access Control (IAM):**
- MenÃº lateral â†’ **â€œAccess control (IAM)â€**
- PestaÃ±as superiores visibles
1. **Verificar permisos especÃ­ficos:**
- SecciÃ³n **â€œMy accessâ€** â†’ BotÃ³n **â€œView my accessâ€**
- Panel derecho: **â€œassignments - rg-proyecto-githubâ€**
1. **Confirmar roles asignados:**

|Role                             |Description                                         |Scope                   |Status     |
|---------------------------------|----------------------------------------------------|------------------------|-----------|
|**Reader**                       |View all resources, but does not allow modifications|Subscription (inherited)|âœ… Requerido|
|**Storage Account Contributor**  |Lets you manage storage accounts and access keys    |This resource           |âœ… Requerido|
|**Storage Blob Data Contributor**|Allows for read, write and delete access to blobs   |This resource           |âœ… Requerido|

#### **ObtenciÃ³n de Credenciales:**

1. **Localizar Storage Account:**
- En el grupo de recursos, buscar recurso tipo **â€œStorage accountâ€**
- Hacer clic en el nombre del Storage Account
1. **Acceder a las claves:**
- MenÃº lateral â†’ **â€œSecurity + networkingâ€** â†’ **â€œAccess keysâ€**
- Copiar **Storage account name**
- Revelar y copiar **Key 1** o **Key 2**

### **1. Configurar Databricks Secrets**

#### **Crear Secret Scope:**

1. **Navegar a secrets:**
   
   ```
   URL: https://tu-workspace.cloud.databricks.com/#secrets/createScope
   ```
1. **ConfiguraciÃ³n:**
- **Scope Name:** `azure-storage-secrets`
- **Manage Principal:** Creator
- **Backend Type:** Databricks

#### **Agregar Secrets:**

1. **Secret 1 - Nombre del Storage:**
   
   ```
   URL: https://tu-workspace.cloud.databricks.com/#secrets/createSecret
   ```
- **Scope:** `azure-storage-secrets`
- **Key:** `storage-account-name`
- **Value:** [Nombre del Storage Account]
1. **Secret 2 - Clave de Acceso:**
   
   ```
   URL: https://tu-workspace.cloud.databricks.com/#secrets/createSecret
   ```
- **Scope:** `azure-storage-secrets`
- **Key:** `storage-account-key`
- **Value:** [Clave completa del Storage Account]

### **2. Configurar ConexiÃ³n en Databricks**

#### **CÃ³digo de ConfiguraciÃ³n (Notebook):**

```python
# Verificar configuraciÃ³n de secrets
print("ğŸ” Verificando configuraciÃ³n de secrets...")
try:
    scopes = dbutils.secrets.listScopes()
    target_scope = "azure-storage-secrets"
    
    if target_scope in [scope.name for scope in scopes]:
        secrets = dbutils.secrets.list(target_scope)
        secret_keys = [secret.key for secret in secrets]
        print(f"âœ… Scope encontrado: {target_scope}")
        print(f"ğŸ”‘ Secrets disponibles: {secret_keys}")
    else:
        print(f"âŒ Scope '{target_scope}' no encontrado")
        
except Exception as e:
    print(f"âŒ Error: {e}")
```

```python
# Configurar cliente de Azure Storage
%pip install azure-storage-blob

from azure.storage.blob import BlobServiceClient

def get_storage_client():
    try:
        # Obtener credenciales desde secrets
        storage_account_name = dbutils.secrets.get(scope="azure-storage-secrets", key="storage-account-name")
        storage_account_key = dbutils.secrets.get(scope="azure-storage-secrets", key="storage-account-key")
        
        # Crear connection string
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net"
        
        # Crear cliente
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        
        # Test de conexiÃ³n
        containers = list(blob_service_client.list_containers())
        print(f"âœ… ConexiÃ³n exitosa! Contenedores: {len(containers)}")
        
        return blob_service_client
        
    except Exception as e:
        print(f"âŒ Error de conexiÃ³n: {e}")
        return None

# Inicializar cliente
storage_client = get_storage_client()
```

### **3. Crear Estructura del Proyecto**

#### **Configurar Medallion Architecture:**

```python
# ConfiguraciÃ³n del proyecto
container_name = "alopez-proyecto-gh"
folders = ["bronze", "silver", "gold"]

def setup_project_structure():
    if not storage_client:
        print("âŒ Cliente no disponible")
        return False
    
    try:
        # Crear contenedor
        try:
            container_client = storage_client.create_container(container_name)
            print(f"âœ… Contenedor '{container_name}' creado")
        except Exception as e:
            if "ContainerAlreadyExists" in str(e):
                print(f"â„¹ï¸  Contenedor '{container_name}' ya existe")
        
        # Crear carpetas (Bronze, Silver, Gold)
        for folder in folders:
            blob_name = f"{folder}/.placeholder"
            blob_client = storage_client.get_blob_client(container=container_name, blob=blob_name)
            blob_client.upload_blob(f"# Carpeta {folder} - Medallion Architecture", overwrite=True)
            print(f"âœ… Carpeta '{folder}' creada")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

# Ejecutar configuraciÃ³n
setup_project_structure()
```

### **4. Verificar ConfiguraciÃ³n Completa**

#### **Checklist de ValidaciÃ³n:**

```python
# VerificaciÃ³n final de la configuraciÃ³n
def verify_complete_setup():
    print("ğŸ” VERIFICACIÃ“N COMPLETA DEL ENTORNO")
    print("=" * 50)
    
    checks = []
    
    # Check 1: Databricks Secrets
    try:
        dbutils.secrets.get("azure-storage-secrets", "storage-account-name")
        checks.append("âœ… Databricks Secrets configurados")
    except:
        checks.append("âŒ Databricks Secrets faltantes")
    
    # Check 2: Azure Storage Connection
    if storage_client:
        checks.append("âœ… ConexiÃ³n a Azure Storage")
    else:
        checks.append("âŒ Sin conexiÃ³n a Azure Storage")
    
    # Check 3: Estructura del proyecto
    try:
        container_client = storage_client.get_container_client(container_name)
        blobs = list(container_client.list_blobs())
        if len(blobs) >= 3:
            checks.append("âœ… Estructura Medallion creada")
        else:
            checks.append("âŒ Estructura Medallion incompleta")
    except:
        checks.append("âŒ Error verificando estructura")
    
    # Mostrar resultados
    for check in checks:
        print(f"   {check}")
    
    all_good = all("âœ…" in check for check in checks)
    
    if all_good:
        print("\nğŸ‰ Â¡ConfiguraciÃ³n completa y exitosa!")
        print("ğŸ“ Listo para la Fase 2: Carga de datos")
    else:
        print("\nâš ï¸  Hay elementos por configurar")
        print("ğŸ“ Revisa los elementos marcados con âŒ")

# Ejecutar verificaciÃ³n
verify_complete_setup()
```

-----

## **Resumen de la Fase 1**

### **âœ… Elementos Configurados:**

1. **Databricks Community Edition**
- âœ… Cuenta creada y verificada
- âœ… Workspace activo
- âœ… Interfaz explorada
1. **ClÃºster Computacional**
- âœ… ClÃºster creado con configuraciÃ³n Ã³ptima
- âœ… Runtime LTS seleccionado
- âœ… Estado activo (cÃ­rculo verde)
1. **Azure Storage Account**
- âœ… Permisos verificados en Portal Azure
- âœ… Credenciales obtenidas
- âœ… Databricks Secrets configurados
- âœ… ConexiÃ³n establecida
1. **Estructura del Proyecto**
- âœ… Contenedor `alopez-proyecto-gh` creado
- âœ… Medallion Architecture implementada:
  - ğŸ¥‰ **Bronze:** Datos en bruto
  - ğŸ¥ˆ **Silver:** Datos procesados
  - ğŸ¥‡ **Gold:** Datos para anÃ¡lisis

### **ğŸ¯ PrÃ³ximo Paso:**

**[â¡ï¸ Fase 2: Carga y Almacenamiento (Capa Bronze)](./fase-2-capa-bronze.md)**

### **ğŸ“š Material de Apoyo:**

- **Databricks:** [DocumentaciÃ³n Oficial](https://docs.databricks.com/getting-started/index.html)
- **Azure Storage:** [GuÃ­a de Python SDK](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)
- **Medallion Architecture:** [Conceptos y mejores prÃ¡cticas](https://databricks.com/glossary/medallion-architecture)

