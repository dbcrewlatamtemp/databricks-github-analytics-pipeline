# **Fase 2: Carga y Almacenamiento (Capa Bronze)**

## **IntroducciÃ³n**

Ahora que nuestro entorno estÃ¡ listo y tenemos configurado Azure Storage Account, vamos a realizar nuestra primera tarea de ingenierÃ­a de datos. Descargaremos archivos de eventos de GitHub y los almacenaremos en su formato original (crudo) en Azure Storage. Esta primera capa de almacenamiento se conoce como **Capa Bronze** en la Medallion Architecture.

-----

## **Paso 2.1: Creando tu Primer Notebook**

El notebook es donde escribiremos y ejecutaremos nuestro cÃ³digo para la ingesta de datos.

### **ConfiguraciÃ³n del Notebook:**

1. **Ve al Workspace:** Haz clic en el Ã­cono de â€œWorkspaceâ€ en el menÃº de la izquierda.
1. **Crea una carpeta:** Es una buena prÃ¡ctica organizar tu trabajo. Crea una carpeta llamada `Proyecto-GitHub-Analytics`.
1. **Crea un nuevo Notebook:**
- Dentro de la carpeta que acabas de crear, haz clic en el botÃ³n de â€œCrearâ€ y selecciona â€œNotebookâ€
- **Nombre:** AsÃ­gnale un nombre descriptivo, como `01_Ingesta_Bronze_Azure`
- **Default Language:** Selecciona Python
- **Cluster:** AsegÃºrate de que estÃ© adjunto al clÃºster que creaste en el paso anterior
1. **Abre el Notebook:** Haz clic en â€œCreateâ€ y se abrirÃ¡ tu primer notebook.

-----

## **Paso 2.2: ConfiguraciÃ³n del Entorno Azure Storage**

Antes de descargar datos, configuraremos la conexiÃ³n a Azure Storage usando los secrets que creamos anteriormente.

### **Celda 1: Importar librerÃ­as y configurar Azure Storage**

Copia y pega el siguiente cÃ³digo en la primera celda del notebook:

```python
# Importar librerÃ­as necesarias
import urllib.request 
import os 
import gzip 
import shutil
import tempfile
from azure.storage.blob import BlobServiceClient
from datetime import datetime

print("ğŸ“¦ CONFIGURACIÃ“N AZURE STORAGE PARA PROYECTO GITHUB")
print("=" * 60)

# Configurar cliente de Azure Storage usando secrets
def get_storage_client():
    try:
        # Obtener credenciales desde Databricks secrets
        storage_account_name = dbutils.secrets.get(scope="azure-storage-secrets", key="storage-account-name")
        storage_account_key = dbutils.secrets.get(scope="azure-storage-secrets", key="storage-account-key")
        
        # Crear connection string
        connection_string = f"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net"
        
        # Crear cliente
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        
        print(f"âœ… Conectado a Azure Storage: {storage_account_name}")
        return blob_service_client
        
    except Exception as e:
        print(f"âŒ Error al conectar con Azure Storage: {e}")
        return None

# Inicializar cliente
storage_client = get_storage_client()

# ConfiguraciÃ³n del proyecto
container_name = "alopez-proyecto-gh"
bronze_folder = "bronze"
silver_folder = "silver" 
gold_folder = "gold"

print(f"ğŸ“ Contenedor: {container_name}")
print(f"ğŸ¥‰ Capa Bronze: {bronze_folder}/")
print(f"ğŸ¥ˆ Capa Silver: {silver_folder}/")
print(f"ğŸ¥‡ Capa Gold: {gold_folder}/")
```

**Para ejecutar una celda, presiona Shift + Enter.**

### **Celda 2: Verificar y configurar la estructura del proyecto**

```python
# Verificar que la estructura del proyecto existe en Azure Storage
def verify_project_structure():
    if not storage_client:
        print("âŒ Cliente de storage no disponible")
        return False
    
    try:
        # Verificar que el contenedor existe
        container_client = storage_client.get_container_client(container_name)
        
        # Verificar que las carpetas existen
        folders_to_check = [bronze_folder, silver_folder, gold_folder]
        existing_folders = []
        
        blobs = container_client.list_blobs()
        blob_names = [blob.name for blob in blobs]
        
        for folder in folders_to_check:
            folder_exists = any(blob_name.startswith(f"{folder}/") for blob_name in blob_names)
            if folder_exists:
                existing_folders.append(folder)
                print(f"âœ… Carpeta '{folder}' verificada")
            else:
                print(f"âš ï¸  Carpeta '{folder}' no encontrada")
        
        if len(existing_folders) == 3:
            print("ğŸ‰ Estructura del proyecto verificada completamente")
            return True
        else:
            print("ğŸ“ Creando carpetas faltantes...")
            create_missing_folders(folders_to_check, existing_folders)
            return True
            
    except Exception as e:
        print(f"âŒ Error verificando estructura: {e}")
        return False

def create_missing_folders(all_folders, existing_folders):
    """Crear carpetas faltantes"""
    missing_folders = [folder for folder in all_folders if folder not in existing_folders]
    
    for folder in missing_folders:
        try:
            blob_name = f"{folder}/.placeholder"
            blob_client = storage_client.get_blob_client(container=container_name, blob=blob_name)
            blob_client.upload_blob(f"# Carpeta {folder} del proyecto GitHub Analytics", overwrite=True)
            print(f"âœ… Carpeta '{folder}' creada")
        except Exception as e:
            print(f"âŒ Error creando carpeta '{folder}': {e}")

# Ejecutar verificaciÃ³n
verify_project_structure()
```

### **Celda 3: Funciones utilitarias para Azure Storage**

```python
# Funciones para gestionar archivos en Azure Storage
class GitHubDataManager:
    def __init__(self, storage_client, container_name):
        self.storage_client = storage_client
        self.container_name = container_name
    
    def upload_file_to_bronze(self, local_file_path, blob_name):
        """Subir archivo a la capa Bronze"""
        try:
            blob_path = f"{bronze_folder}/{blob_name}"
            blob_client = self.storage_client.get_blob_client(container=self.container_name, blob=blob_path)
            
            with open(local_file_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)
            
            print(f"âœ… Archivo subido a Bronze: {blob_name}")
            return True
            
        except Exception as e:
            print(f"âŒ Error subiendo archivo: {e}")
            return False
    
    def list_bronze_files(self):
        """Listar archivos en la capa Bronze"""
        try:
            container_client = self.storage_client.get_container_client(self.container_name)
            blobs = container_client.list_blobs(name_starts_with=f"{bronze_folder}/")
            
            files = []
            print(f"ğŸ“ Archivos en la capa Bronze:")
            for blob in blobs:
                if not blob.name.endswith('.placeholder'):
                    files.append(blob.name)
                    file_size = blob.size / (1024*1024)  # MB
                    print(f"  ğŸ“„ {blob.name} ({file_size:.2f} MB)")
            
            return files
            
        except Exception as e:
            print(f"âŒ Error listando archivos: {e}")
            return []
    
    def get_bronze_file_info(self):
        """Obtener informaciÃ³n detallada de archivos en Bronze"""
        try:
            container_client = self.storage_client.get_container_client(self.container_name)
            blobs = container_client.list_blobs(name_starts_with=f"{bronze_folder}/")
            
            total_size = 0
            file_count = 0
            
            for blob in blobs:
                if not blob.name.endswith('.placeholder'):
                    total_size += blob.size
                    file_count += 1
            
            total_size_mb = total_size / (1024*1024)
            print(f"ğŸ“Š Resumen Capa Bronze:")
            print(f"   Archivos: {file_count}")
            print(f"   TamaÃ±o total: {total_size_mb:.2f} MB")
            
            return {"file_count": file_count, "total_size_mb": total_size_mb}
            
        except Exception as e:
            print(f"âŒ Error obteniendo informaciÃ³n: {e}")
            return None

# Crear instancia del gestor
data_manager = GitHubDataManager(storage_client, container_name)

print("ğŸ”§ GitHubDataManager inicializado")
print("ğŸ“ Funciones disponibles:")
print("   - data_manager.upload_file_to_bronze(archivo_local, nombre_blob)")
print("   - data_manager.list_bronze_files()")
print("   - data_manager.get_bronze_file_info()")
```

-----

## **Paso 2.3: CÃ³digo para Descargar y Guardar los Datos en Azure Storage**

Ahora descargaremos los datos de eventos de GitHub y los almacenaremos en la capa Bronze de Azure Storage.

### **Celda 4: Configurar URLs de descarga de GitHub Archive**

```python
# ConfiguraciÃ³n de descarga de datos de GitHub Archive
print("ğŸ™ CONFIGURACIÃ“N DE DESCARGA GITHUB ARCHIVE")
print("=" * 50)

# Lista de fechas y horas para las que queremos descargar los datos
# Formato: AAAA-MM-DD-H
# Descargamos las primeras 3 horas de dos dÃ­as diferentes como ejemplo
urls_to_download = [
    "https://data.gharchive.org/2025-01-01-0.json.gz",
    "https://data.gharchive.org/2025-01-01-1.json.gz", 
    "https://data.gharchive.org/2025-01-01-2.json.gz",
    "https://data.gharchive.org/2025-01-02-0.json.gz",
    "https://data.gharchive.org/2025-01-02-1.json.gz",
    "https://data.gharchive.org/2025-01-02-2.json.gz"
]

print(f"ğŸ“… Archivos a descargar: {len(urls_to_download)}")
print("ğŸ“‹ Lista de archivos:")
for i, url in enumerate(urls_to_download, 1):
    file_name = url.split("/")[-1]
    print(f"   {i}. {file_name}")

print(f"\nğŸ’¾ Destino: Azure Storage â†’ {container_name} â†’ {bronze_folder}/")
```

### **Celda 5: FunciÃ³n de descarga y carga a Azure Storage**

```python
# FunciÃ³n para descargar y subir archivos a Azure Storage
def download_and_upload_github_data(urls_list):
    """
    Descarga archivos de GitHub Archive y los sube a Azure Storage (capa Bronze)
    """
    if not storage_client:
        print("âŒ Cliente de Azure Storage no disponible")
        return
    
    successful_downloads = 0
    failed_downloads = 0
    
    print("ğŸš€ INICIANDO PROCESO DE DESCARGA Y CARGA")
    print("=" * 60)
    
    for i, url in enumerate(urls_list, 1):
        file_name_gz = url.split("/")[-1]
        file_name_json = file_name_gz.replace(".gz", "")
        
        print(f"\nğŸ“¦ Procesando archivo {i}/{len(urls_list)}: {file_name_gz}")
        
        # Crear archivos temporales
        with tempfile.NamedTemporaryFile(suffix='.gz', delete=False) as temp_gz:
            temp_gz_path = temp_gz.name
            
        with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as temp_json:
            temp_json_path = temp_json.name
        
        try:
            # Paso 1: Descargar archivo comprimido
            print(f"   ğŸ“¥ Descargando desde: {url}")
            urllib.request.urlretrieve(url, temp_gz_path)
            print(f"   âœ… Descarga completada")
            
            # Paso 2: Descomprimir archivo
            print(f"   ğŸ“‚ Descomprimiendo archivo...")
            with gzip.open(temp_gz_path, 'rb') as f_in:
                with open(temp_json_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            print(f"   âœ… DescompresiÃ³n completada")
            
            # Paso 3: Subir a Azure Storage (capa Bronze)
            print(f"   â˜ï¸  Subiendo a Azure Storage...")
            success = data_manager.upload_file_to_bronze(temp_json_path, file_name_json)
            
            if success:
                successful_downloads += 1
                print(f"   ğŸ‰ {file_name_json} procesado exitosamente")
            else:
                failed_downloads += 1
                print(f"   âŒ Error subiendo {file_name_json}")
            
        except Exception as e:
            failed_downloads += 1
            print(f"   âŒ Error procesando {file_name_gz}: {e}")
        
        finally:
            # Limpiar archivos temporales
            try:
                os.unlink(temp_gz_path)
                os.unlink(temp_json_path)
            except:
                pass
    
    # Resumen final
    print("\n" + "=" * 60)
    print("ğŸ“Š RESUMEN DE DESCARGA")
    print("=" * 60)
    print(f"âœ… Descargas exitosas: {successful_downloads}")
    print(f"âŒ Descargas fallidas: {failed_downloads}")
    print(f"ğŸ“ Total de archivos: {len(urls_list)}")
    
    if successful_downloads > 0:
        print(f"\nğŸ‰ Â¡Proceso completado! Los datos estÃ¡n en Azure Storage.")
        print(f"ğŸ“ UbicaciÃ³n: {container_name}/{bronze_folder}/")
    
    return successful_downloads, failed_downloads

# Ejecutar el proceso de descarga
results = download_and_upload_github_data(urls_to_download)
```

-----

## **Paso 2.4: Verificar que los Archivos se Guardaron Correctamente**

### **Celda 6: VerificaciÃ³n de archivos en Azure Storage**

```python
# Verificar que los archivos se guardaron correctamente en Azure Storage
print("ğŸ” VERIFICACIÃ“N DE ARCHIVOS EN CAPA BRONZE")
print("=" * 50)

# Listar archivos en la capa Bronze
bronze_files = data_manager.list_bronze_files()

# Obtener informaciÃ³n detallada
file_info = data_manager.get_bronze_file_info()

# VerificaciÃ³n adicional desde Azure Portal
print(f"\nğŸ”— Para verificar en Azure Portal:")
print(f"   1. Ve a: https://portal.azure.com")
print(f"   2. Navega a: rg-proyecto-github â†’ Storage Account")
print(f"   3. Ve a: Containers â†’ {container_name} â†’ {bronze_folder}/")
print(f"   4. DeberÃ­as ver {len(bronze_files)} archivos .json")

# ValidaciÃ³n de contenido (sample de un archivo)
if bronze_files:
    print(f"\nğŸ“„ ValidaciÃ³n de contenido del primer archivo:")
    try:
        # Descargar una muestra pequeÃ±a del primer archivo para verificar
        first_file = bronze_files[0]
        blob_client = storage_client.get_blob_client(container=container_name, blob=first_file)
        
        # Leer las primeras lÃ­neas
        sample_data = blob_client.download_blob().readall()
        sample_lines = sample_data.decode('utf-8').split('\n')[:3]
        
        print(f"   Archivo: {first_file.split('/')[-1]}")
        print(f"   Primeras lÃ­neas de ejemplo:")
        for i, line in enumerate(sample_lines, 1):
            if line.strip():
                print(f"     {i}. {line[:100]}...")
        
        print("   âœ… El archivo contiene datos JSON vÃ¡lidos de GitHub")
        
    except Exception as e:
        print(f"   âŒ Error verificando contenido: {e}")

print(f"\nğŸ‰ Â¡Felicidades! Has completado la carga de datos a la Capa Bronze.")
print(f"ğŸ“ˆ PrÃ³ximo paso: Transformar estos datos en la Capa Silver.")
```

### **Celda 7: FunciÃ³n utilitaria para futuros notebooks**

```python
# FunciÃ³n utilitaria para reutilizar en otros notebooks
def get_bronze_file_paths():
    """
    Obtener las rutas de todos los archivos en la capa Bronze
    Ãštil para los siguientes notebooks de transformaciÃ³n
    """
    try:
        container_client = storage_client.get_container_client(container_name)
        blobs = container_client.list_blobs(name_starts_with=f"{bronze_folder}/")
        
        file_paths = []
        for blob in blobs:
            if not blob.name.endswith('.placeholder'):
                # Formato para leer desde Azure Storage en Spark
                azure_path = f"abfss://{container_name}@{storage_client.account_name}.dfs.core.windows.net/{blob.name}"
                file_paths.append(azure_path)
        
        return file_paths
        
    except Exception as e:
        print(f"âŒ Error obteniendo rutas: {e}")
        return []

# Guardar informaciÃ³n para el siguiente notebook
bronze_paths = get_bronze_file_paths()
print(f"ğŸ“ Rutas para el siguiente notebook:")
for path in bronze_paths[:3]:  # Mostrar solo las primeras 3
    print(f"   {path}")

if len(bronze_paths) > 3:
    print(f"   ... y {len(bronze_paths) - 3} archivos mÃ¡s")

print(f"\nğŸ’¡ Estas rutas se usarÃ¡n en el notebook de transformaciÃ³n (Silver)")
```

-----

## **Â¿QuÃ© es GitHub Archive?**

**GitHub Archive** es un proyecto que registra la cronologÃ­a pÃºblica de GitHub, capturando eventos que ocurren en repositorios pÃºblicos cada hora.

### **Tipos de Eventos Capturados:**

|Evento              |DescripciÃ³n                             |Frecuencia TÃ­pica|
|--------------------|----------------------------------------|-----------------|
|**PushEvent**       |Commits subidos a repositorios          |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥            |
|**CreateEvent**     |CreaciÃ³n de ramas, tags o repositorios  |ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª            |
|**PullRequestEvent**|Eventos de Pull Requests                |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª            |
|**IssuesEvent**     |Apertura, cierre o comentarios en issues|ğŸ”¥ğŸ”¥ğŸ”¥âšªâšª            |
|**WatchEvent**      |Usuario da â€œstarâ€ a un repositorio      |ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥âšª            |
|**ForkEvent**       |Fork de un repositorio                  |ğŸ”¥ğŸ”¥âšªâšªâšª            |

### **Estructura de Datos:**

Cada archivo JSON contiene eventos con informaciÃ³n como:

- Usuario que realizÃ³ la acciÃ³n
- Repositorio afectado
- Tipo de evento
- Timestamp del evento
- Detalles especÃ­ficos del evento

### **Ejemplo de Evento JSON:**

```json
{
  "id": "12345678901",
  "type": "PushEvent",
  "actor": {
    "id": 123456,
    "login": "octocat",
    "display_login": "octocat"
  },
  "repo": {
    "id": 654321,
    "name": "octocat/Hello-World"
  },
  "created_at": "2025-01-01T00:00:00Z",
  "payload": {
    "commits": [
      {
        "sha": "abc123def456...",
        "message": "Update README.md"
      }
    ]
  }
}
```

-----

## **Resumen de la Fase 2**

### **âœ… Lo que has logrado:**

1. **Notebook de Ingesta Creado**
- âœ… ConfiguraciÃ³n de Azure Storage
- âœ… Funciones utilitarias implementadas
- âœ… Manejo de errores incluido
1. **Datos de GitHub Descargados**
- âœ… 6 archivos de eventos de GitHub (2 dÃ­as, 3 horas cada uno)
- âœ… Datos descomprimidos y almacenados en formato JSON
- âœ… Aproximadamente 6-20 MB de datos por archivo
1. **Capa Bronze Implementada**
- âœ… Datos en formato crudo/original
- âœ… Almacenados en Azure Storage
- âœ… Estructura organizada por fecha/hora
1. **Funcionalidades Disponibles**
- âœ… GestiÃ³n automÃ¡tica de archivos temporales
- âœ… ValidaciÃ³n de contenido
- âœ… InformaciÃ³n estadÃ­stica de archivos
- âœ… PreparaciÃ³n para siguiente fase

### **ğŸ¯ PrÃ³ximos Pasos:**

**Semana 2:** TransformaciÃ³n de datos y creaciÃ³n de la Capa Silver

- Limpieza y validaciÃ³n de datos
- AplicaciÃ³n de esquemas estructurados
- Transformaciones con PySpark
- OptimizaciÃ³n de performance

### **ğŸ“š Material de Apoyo:**

- **GitHub Archive:** [DocumentaciÃ³n oficial](https://www.gharchive.org/)
- **Azure Blob Storage:** [GuÃ­a de Python SDK](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)
- **Medallion Architecture:** [Mejores prÃ¡cticas](https://databricks.com/glossary/medallion-architecture)

### **ğŸ” Troubleshooting ComÃºn:**

|Problema                  |SoluciÃ³n                                                                    |
|--------------------------|----------------------------------------------------------------------------|
|Error de conexiÃ³n a Azure |Verificar secrets y permisos                                                |
|Archivos no se descargan  |Verificar conectividad a internet                                           |
|Error de espacio temporal |Los archivos temporales se limpian automÃ¡ticamente                          |
|Error de permisos en Azure|Verificar roles: Storage Account Contributor + Storage Blob Data Contributor|

-----

**ğŸ‰ Â¡Felicidades!** Has completado exitosamente la **Fase 2** del proyecto. Tienes una capa Bronze funcional con datos reales de GitHub listos para ser transformados en la siguiente semana.

**[â¬…ï¸ Regresar a Fase 1](./fase-1-configuracion.md)** | **[â¡ï¸ Continuar a Semana 2](../semana-2/README.md)** *(PrÃ³ximamente)*

